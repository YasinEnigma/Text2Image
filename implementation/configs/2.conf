# All paths to different required data objects
images_dir: "../data/LFW/lfw"
processed_text_file: "processed_annotations/processed_text.pkl"
annotations_file: "../data/LFW/Face2Text/face2text_v0.1/clean.json"
pretrained_encoder_file: "networks/InferSent/models/infersent2.pkl"
pretrained_embedding_file: "networks/InferSent/models/glove.840B.300d.txt"

# Training specific directories
log_dir: "training_runs/2/losses/"
sample_dir: "training_runs/2/generated_samples/"
save_dir: "training_runs/2/saved_models/"

# Hyperparameters for the Model
captions_length: 100
img_dims:
  - 64
  - 64

# whether to use a pretrained encoder:
use_pretrained_encoder: True

# LSTM hyperparameters
# embedding_size: 128
# hidden_size: 512
# num_layers: 3  # number of LSTM cells in the encoder network

# Conditioning Augmentation hyperparameters
hidden_size: 4096  # output size of the Pretrained encoder
ca_out_size: 128
compressed_latent_size: 32

# Pro GAN hyperparameters
use_eql: True
use_ema: True
ema_decay: 0.999
depth: 5
latent_size: 256
learning_rate: 0.001
beta_1: 0
beta_2: 0.99
eps: 0.00000001
drift: 0.001
n_critic: 1

# Training hyperparameters:
epochs:
  - 120
  - 120
  - 120
  - 120
  - 120

# % of epochs for fading in the new layer
fade_in_percentage:
  - 50
  - 50
  - 50
  - 50
  - 50

batch_sizes:
  - 64
  - 64
  - 64
  - 64
  - 32

loss_function: "wgan-gp"  # the loss function to be used
num_workers: 3
feedback_factor: 1  # number of logs generated per epoch
checkpoint_factor: 10 # save the models after these many epochs
use_matching_aware_discriminator: True  # use the matching aware discriminator
